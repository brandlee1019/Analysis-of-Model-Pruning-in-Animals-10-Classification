# Analysis-of-Model-Pruning-in-Animals-10-Classification
[ä¸­æ–‡](#ä¸­æ–‡) | [English](#english)

<a id="english"></a>
## ğŸŒŸ Project Overview
This project explores the benefits of model pruning techniques (weight pruning and filter pruning) on CNN architectures for animal image classification using the Animals-10 dataset. We compare performance and efficiency between:
- Custom CNN model
- Transfer learning model (EfficientNet B7)
- Their pruned variants

Key Findings:
- Achieved **98.94% size reduction** with weight pruning on custom CNN
- Filter pruning boosted custom CNN accuracy by **6.86%** at 30% sparsity
- Transfer learning model maintained **95% accuracy** with 0.46% size reduction

<a id="ä¸­æ–‡"></a>
## ğŸŒŸ é¡¹ç›®æ¦‚è¿°
æœ¬é¡¹ç›®ä½¿ç”¨Animals-10æ•°æ®é›†ï¼Œæ¢ç´¢æ¨¡å‹å‰ªææŠ€æœ¯(æƒé‡å‰ªæå’Œè¿‡æ»¤å™¨å‰ªæ)åœ¨CNNæ¶æ„ä¸­çš„æ•ˆæœã€‚æˆ‘ä»¬æ¯”è¾ƒäº†ï¼š
- è‡ªå®šä¹‰CNNæ¨¡å‹
- è¿ç§»å­¦ä¹ æ¨¡å‹(EfficientNet B7)
- å‰ªæåå˜ä½“

æ ¸å¿ƒå‘ç°ï¼š
- è‡ªå®šä¹‰CNNæƒé‡å‰ªæå®ç°**98.94%æ¨¡å‹å‹ç¼©**
- è¿‡æ»¤å™¨å‰ªæåœ¨30%ç¨€ç–åº¦ä¸‹æå‡å‡†ç¡®ç‡**6.86%**
- è¿ç§»å­¦ä¹ æ¨¡å‹ä¿æŒ**95%å‡†ç¡®ç‡**åŒæ—¶å‡å°0.46%ä½“ç§¯

## ğŸš€ Key Features
**Bilingual Support** | **åŒè¯­è¨€æ”¯æŒ**  
ğŸ“Š Comparative analysis of pruning techniques  
ğŸ¦¾ Resource-efficient model deployment solutions  
ğŸ“ˆ Detailed metrics visualization

## ğŸ› ï¸ Methodology
### Technical Framework
| Component        | Specifications                          |
|------------------|-----------------------------------------|
| Base Models      | Custom CNN, EfficientNet B7            |
| Pruning Methods  | Weight Pruning, Filter Pruning         |
| Sparsity Levels  | 30%, 50%, 70%, 90%                     |
| Training Epochs  | 30-100 (with early stopping)           |

### æŠ€æœ¯æ¡†æ¶
| ç»„ä»¶            | è§„æ ¼å‚æ•°                               |
|-----------------|---------------------------------------|
| åŸºç¡€æ¨¡å‹        | è‡ªå®šä¹‰CNN, EfficientNet B7           |
| å‰ªææ–¹æ³•        | æƒé‡å‰ªæ, è¿‡æ»¤å™¨å‰ªæ                  |
| ç¨€ç–åº¦          | 30%, 50%, 70%, 90%                   |
| è®­ç»ƒå‘¨æœŸ        | 30-100 (å«æ—©åœæœºåˆ¶)                  |

## ğŸ“Š Results Highlights
### Model Comparison
| Model                   | Accuracy | Size  | Compression |
|-------------------------|----------|-------|-------------|
| Transfer Learning       | 96.98%   | 226MB | -           |
| Weight Pruned Custom CNN| 79.18%   | 0.56MB| 98.94%      |
| Filter Pruned Custom CNN| 83.20%   | 39.8MB| 30.76%      |

### æ ¸å¿ƒç»“æœ
| æ¨¡å‹                  | å‡†ç¡®ç‡   | å¤§å°   | å‹ç¼©ç‡     |
|-----------------------|---------|--------|-----------|
| è¿ç§»å­¦ä¹ æ¨¡å‹          | 96.98%  | 226MB  | -         |
| æƒé‡å‰ªæCNN           | 79.18%  | 0.56MB | 98.94%    |
| è¿‡æ»¤å™¨å‰ªæCNN         | 83.20%  | 39.8MB | 30.76%    |

## ğŸ§  Key Insights
1. **Pruning Paradox**: Moderate sparsity (30-70%) often improves model performance
2. **Storage Optimization**: Weight pruning + gzip achieves extreme compression
3. **Architecture Matters**: Filter pruning works better with shallow networks

## æ ¸å¿ƒæ´è§
1. **å‰ªææ‚–è®º**ï¼šé€‚åº¦ç¨€ç–åº¦(30-70%)å¸¸æå‡æ¨¡å‹è¡¨ç°  
2. **å­˜å‚¨ä¼˜åŒ–**ï¼šæƒé‡å‰ªæ+gzipå®ç°æè‡´å‹ç¼©  
3. **æ¶æ„å·®å¼‚**ï¼šè¿‡æ»¤å™¨å‰ªææ›´é€‚ç”¨äºæµ…å±‚ç½‘ç»œ  

## ğŸ“š References
- Dataset: [Animals-10 on Kaggle](https://www.kaggle.com/datasets/alessiocorrado99/animals10)
- Pruning Implementation: [Keras Pruning API](https://www.tensorflow.org/model_optimization)
- Base Model: [EfficientNet B7](https://keras.io/api/applications/efficientnet/)

## ğŸ“œ License
MIT License - See [LICENSE](LICENSE) for details
